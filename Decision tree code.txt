Aim-Practical of Decision Tree.

Theory!-

- A Decision Tree is an algorithm used for Supervised learning problems such as classification or regression. A Decision or classification tree is a tree in which each internal (nonleaf) node is labeled with an input feature.

The arcs coming from a node labeled with feature are labeled with each of the possible Values the feature. Each leaf of the tree is labeled with a class or probability distribution onex the classes.

A tree Can be learned" by splitting the Source into subsets based on an attribute value test. This process is repeated on each dexined subset in a recursive manner Called recursive partitioning.

-This process of top-Down induction of decision tree is an example of greedy algorithm, and it is the most Common Strategy for learning decision trees. Decision tree used in data mining are of two main types

• classification tree:-

when the response is nominal variable, for example if an email
Regression tree:

when the predicted outcome can be Considered a real number.

Decision trees are a simple method, and as such has some problems. One of this issues is a high variance in the resulting models that decision tree produce, In order to alleviate this Problem, ensemble methods of decision tree were dendoped.

There are two groups of ensemble methods currently used extensively.
Bagging decision trees: These trees are used to build multiple decision trees by repeatedl sesampling training data with replacement, f voting the trees for a Consensus Prediction.

Boosting decision tree: Gradient boosting Combines weak learners in this case, decision trees into a single strong, fashion. It fits learner, in an iterative a weak tree to the data and iteratively keeps fitting weak learners in order to correct the Previous model. error of the





"PassengerId:type should be integers
Survived:Survived or Not
Pclass:Class of Travel
Name:Name of Passenger
Sex:Gender
Age:Age of Passengers
SibSp:Number of Sibling/Spouse aboard
Parch:Number of Parent/Child aboard
Ticket
Fare
Cabin
Embarked:The port in which a passenger has embarked. C - Cherbourg, S - Southampton,
Q = Queenstown"
titanic<-read.csv(file.choose(), header=TRUE, sep=",")
summary(titanic)
names(titanic)
install.packages("partykit")
install.packages("CHAID",repos="http://R-Forge.R-project.org", type="source")
library(CHAID)
library(partykit)
titanic$Survived<-as.factor(titanic$Survived)
summary(titanic$Survived)
names(titanic)
tree<-chaid(formula=Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked, data=titanic)
class(titanic$Survived)
library(rpart)
fit<-rpart(Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked, data=titanic,
           method="class")
plot(fit)
text(fit)
install.packages('rattle')
install.packages('rpart.plot')
install.packages('RColorBrewer')
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(fit)
Prediction<-predict(fit, titanic, type="class")
Prediction
hitters<-read.csv(file.choose(), sep=",", header=TRUE)
summary(hitters)
reg.tree<-rpart(Salary~Years+Hits, data=hitters)
rpart.plot(reg.tree, type=4)
reg.tree$variable.importance
install.packages("MASS")
library(MASS)
set.seed(1984)
library(rpart)
train<-sample(1:nrow(hitters), nrow(hitters)/2)
tree_baseball<-rpart(Salary~Hits+HmRun+Runs+RBI+Walks+Years+Errors, subset=train,
                     data=hitters)
library(rpart.plot)
rpart.plot(tree_baseball)
tree_baseball$variable.importance

